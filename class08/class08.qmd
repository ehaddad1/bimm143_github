---
title: "Class 8: Mini Project"
author: "Eli Haddad (A16308227)"
format: pdf
---

Today we will apply the machine learning methods we introduced in the last class on breast cancer biopsy data from fine needle aspiration (FNA).


## Data input
The data is supplied on CSV format:

```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)

head(wisc.df)
```
Now I will store the diagnosis column for later and exclude it from the data set I will actually do things with that I will call `wisc.data`


```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
wisc.data <- wisc.df[,-1]
```

>Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

>Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

>Q3. How many variables/features in the data are suffixed with _mean?

```{r}
x <- colnames(wisc.df)
length(grep("_mean$", x))
```

# 2. Principal Component Analysis

We need to scale our input data before PCA as some of the columns are measured in terms of very different units with different means and different variances. The upshot here is we set `scale=TRUE` argument to `prcomp()`.

```{r}
wisc.pr <- prcomp(wisc.data, scale =TRUE)
summary(wisc.pr)
```
>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

From my results, 44.27% of the original variance is captured by PC1.

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

Three principal components are required to describe at least 70% of the original variance in the data (The cumulative proportion at PC3 is 72.64%)

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

Seven principal components are required to describe at least 90% of the original variance in the data (The cumulative proportion at PC7 is 91.01%)



Visualizing my PCA results with a biplot
```{r}
biplot(wisc.pr)

```




>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

What stands out to me about this plot is that there were points of both diagnoses plotted, however, it is very hard to see if there is a relationship because a lot of points and names are overlapping. The plot is very messy, making it difficult to understand.  

Scatter plot for PC1 and PC2
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis,pch=16, xlab="PC1", ylab="PC2")
```
Scatter plot for PC1 and PC3
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis,pch=16, xlab="PC1", ylab="PC3")
```
>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

The similarity I notice about these plots are that there is evident clustering between the two diagnoses. In addition, there is a greater separation of clusters in the PC1 and PC2 plot versus the PC1 and PC3 plot.


A more fancy figure of these results:
```{r}
library(ggplot2)
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```
# Variance explained

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve <- pr.var / sum(pr.var)

plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

```
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)

```

# Communicating PCA results

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation[,1]["concave.points_mean"]
```

>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

The minimum number of principal components required to explain 80% of the variance of the data are five. (Cumulative proportion at PC5 is 84.73%)


# 3. Hierarchial clustering

```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist, method = "complete")
```

>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```
The height at which the clustering model has 4 clusters is at height 19.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust,k=4)
table(wisc.hclust.clusters, diagnosis)
```
>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10

```{r}
wisc.hclust.clusters.better <- cutree(wisc.hclust,k=4)
table(wisc.hclust.clusters.better, diagnosis)
```

I did not find a better cluster vs diagnoses match. At clusters 2-3, B and M are clustered into the same group which is not desired. At clusters 4-7, B and M are clustered into two distinct groups, so the lowest cluster (4) is ideal. At clusters 8-9, the M group clusters into multiple groups which is not desired.


>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

```{r}
wisc.hclust <- hclust(data.dist, method = "ward.D2")
plot(wisc.hclust)
```
"ward.D2" is my favorite for the same data.dist dataset because it makes very clear, symmetrical, and clean clustering. The dendogram has a lot of "field goal" line depictions, which is ideal.

# 4. Optional: K-Means clustering

```{r}
wisc.km <- kmeans(data.scaled, centers= 2, nstart= 20)
table(wisc.km$cluster, diagnosis)
```

>Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

```{r}
wisc.km <- kmeans(data.scaled, centers= 2, nstart= 20)
table(wisc.km$cluster, wisc.hclust.clusters)

```
The k-means spearates the diagnoses pretty well. Two distnct clusters can be seen when tabling wisc.km$cluster and diagnosis. It is comparable to the hclust results, except that the hclust results had to create 4 clusters in order to achieve the same result. Cluster 1 from the k-means algorithm can be interpreted as the cluster equivalent to Cluster 1 from the hclust algorithm, while Cluster 2 from the k-means algorithm can be interpreted as the cluster equivalent to Cluster 3 from the hclust algorithm.


# 5. Combining methods

This approach will take not original data, but our PCA results and work with them.

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method = "ward.D2")
plot(wisc.pr.hclust)
```

Generate 2 cluster groups from this hclust object.

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
```



```{r}
table(grps)
```

```{r}
table(diagnosis)
```

```{r}
table(diagnosis, grps)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

Re-ordering the colors so they are more comparable

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

Plotting with the re-ordered factor

```{r}
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)

table(wisc.pr.hclust.clusters, diagnosis)
```
>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

The newly created model with four clusters separates out the two diagnoses very well. It is very distinct to see that B and M are in two different clusters based on the table.

